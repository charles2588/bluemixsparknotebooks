{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":5,"description":"parquet at MultiStringIndexer.scala:255","details":"org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547)\ncom.ibm.analytics.wml.features.adp.MultiStringIndexerModel$$anon$1.saveImpl(MultiStringIndexer.scala:255)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\norg.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:254)\norg.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:253)\nscala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\nscala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\norg.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:253)\norg.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:338)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\ncom.ibm.analytics.wml.features.adp.CategoryEncoderModel$$anon$1.saveImpl(CategoryEncoder.scala:164)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\norg.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:254)\norg.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:253)\nscala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\nscala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\norg.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:253)\norg.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:338)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\ncom.ibm.analytics.wml.features.ADPModel$$anon$2.saveImpl(ADP.scala:447)","physicalPlanDescription":"== Parsed Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data), ErrorIfExists, [fieldName#6317, labels#6318]\n+- AnalysisBarrier\n      +- LocalRelation <empty>, [fieldName#6317, labels#6318]\n\n== Analyzed Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data), ErrorIfExists, [fieldName#6317, labels#6318]\n+- LocalRelation <empty>, [fieldName#6317, labels#6318]\n\n== Optimized Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data), ErrorIfExists, [fieldName#6317, labels#6318]\n+- LocalRelation <empty>, [fieldName#6317, labels#6318]\n\n== Physical Plan ==\nExecute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data), ErrorIfExists, [fieldName#6317, labels#6318]\n+- LocalTableScan <empty>, [fieldName#6317, labels#6318]","sparkPlanInfo":{"nodeName":"Execute InsertIntoHadoopFsRelationCommand","simpleString":"Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/2_CatEncode-Model_d5372f30e7fe/catencode_model/stages/0_multiStrIdx_bcbe97721e25/data), ErrorIfExists, [fieldName#6317, labels#6318]","children":[{"nodeName":"LocalTableScan","simpleString":"LocalTableScan <empty>, [fieldName#6317, labels#6318]","children":[],"metrics":[{"name":"number of output rows","accumulatorId":1535,"metricType":"sum"}]}],"metrics":[{"name":"number of written files","accumulatorId":1531,"metricType":"sum"},{"name":"bytes of written output","accumulatorId":1532,"metricType":"sum"},{"name":"number of output rows","accumulatorId":1533,"metricType":"sum"},{"name":"number of dynamic part","accumulatorId":1534,"metricType":"sum"}]}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":5}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":6,"description":"parquet at StandardScaler.scala:199","details":"org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547)\norg.apache.spark.ml.feature.StandardScalerModel$StandardScalerModelWriter.saveImpl(StandardScaler.scala:199)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\norg.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:254)\norg.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:253)\nscala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\nscala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\norg.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:253)\norg.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:338)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\ncom.ibm.analytics.wml.features.ADPModel$$anon$2.saveImpl(ADP.scala:447)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter.com$ibm$analytics$wml$pipeline$spark$formats$DefaultPipelineModelFormatWriter$$writePipelineModelNode(pipelinemodel.scala:43)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter$$anonfun$3.apply(pipelinemodel.scala:65)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter$$anonfun$3.apply(pipelinemodel.scala:64)\nscala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:683)\nscala.collection.immutable.Map$Map3.foreach(Map.scala:161)\nscala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:682)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter.write(pipelinemodel.scala:64)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter.write(pipelinemodel.scala:26)","physicalPlanDescription":"== Parsed Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data), ErrorIfExists, [std#6323, mean#6324]\n+- AnalysisBarrier\n      +- Repartition 1, true\n         +- LocalRelation [std#6323, mean#6324]\n\n== Analyzed Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data), ErrorIfExists, [std#6323, mean#6324]\n+- Repartition 1, true\n   +- LocalRelation [std#6323, mean#6324]\n\n== Optimized Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data), ErrorIfExists, [std#6323, mean#6324]\n+- Repartition 1, true\n   +- LocalRelation [std#6323, mean#6324]\n\n== Physical Plan ==\nExecute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data), ErrorIfExists, [std#6323, mean#6324]\n+- Exchange RoundRobinPartitioning(1)\n   +- LocalTableScan [std#6323, mean#6324]","sparkPlanInfo":{"nodeName":"Execute InsertIntoHadoopFsRelationCommand","simpleString":"Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeADP/adp_model/stages/4_stdScal_203faa5bd34d/data), ErrorIfExists, [std#6323, mean#6324]","children":[{"nodeName":"Exchange","simpleString":"Exchange RoundRobinPartitioning(1)","children":[{"nodeName":"LocalTableScan","simpleString":"LocalTableScan [std#6323, mean#6324]","children":[],"metrics":[{"name":"number of output rows","accumulatorId":1642,"metricType":"sum"}]}],"metrics":[{"name":"data size total (min, med, max)","accumulatorId":1637,"metricType":"size"}]}],"metrics":[{"name":"number of written files","accumulatorId":1638,"metricType":"sum"},{"name":"bytes of written output","accumulatorId":1639,"metricType":"sum"},{"name":"number of output rows","accumulatorId":1640,"metricType":"sum"},{"name":"number of dynamic part","accumulatorId":1641,"metricType":"sum"}]}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":6}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":7,"description":"parquet at LogisticRegression.scala:1233","details":"org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547)\norg.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelWriter.saveImpl(LogisticRegression.scala:1233)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter.com$ibm$analytics$wml$pipeline$spark$formats$DefaultPipelineModelFormatWriter$$writePipelineModelNode(pipelinemodel.scala:43)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter$$anonfun$3.apply(pipelinemodel.scala:65)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter$$anonfun$3.apply(pipelinemodel.scala:64)\nscala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:683)\nscala.collection.immutable.Map$Map3.foreach(Map.scala:161)\nscala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:682)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter.write(pipelinemodel.scala:64)\ncom.ibm.analytics.wml.pipeline.spark.formats.DefaultPipelineModelFormatWriter.write(pipelinemodel.scala:26)\ncom.ibm.analytics.wml.pipeline.spark.MLPipelineModel$$anon$1.saveImpl(MLPipelineModel.scala:182)\norg.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:103)\ncom.ibm.analytics.ngp.repository_v3.SparkPipelineModelArtifact$SparkPipelineModelReader$$anonfun$com$ibm$analytics$ngp$repository_v3$SparkPipelineModelArtifact$SparkPipelineModelReader$$savePipelineArchive$1.apply$mcV$sp(MLRepositoryArtifact.scala:892)\ncom.ibm.analytics.ngp.repository_v3.SparkPipelineModelArtifact$SparkPipelineModelReader$$anonfun$com$ibm$analytics$ngp$repository_v3$SparkPipelineModelArtifact$SparkPipelineModelReader$$savePipelineArchive$1.apply(MLRepositoryArtifact.scala:892)\ncom.ibm.analytics.ngp.repository_v3.SparkPipelineModelArtifact$SparkPipelineModelReader$$anonfun$com$ibm$analytics$ngp$repository_v3$SparkPipelineModelArtifact$SparkPipelineModelReader$$savePipelineArchive$1.apply(MLRepositoryArtifact.scala:892)\nscala.util.Try$.apply(Try.scala:192)\ncom.ibm.analytics.ngp.repository_v3.SparkPipelineModelArtifact$SparkPipelineModelReader.com$ibm$analytics$ngp$repository_v3$SparkPipelineModelArtifact$SparkPipelineModelReader$$savePipelineArchive(MLRepositoryArtifact.scala:892)\ncom.ibm.analytics.ngp.repository_v3.SparkPipelineModelArtifact$SparkPipelineModelReader$$anonfun$openStream$5$$anonfun$apply$13.apply(MLRepositoryArtifact.scala:918)\ncom.ibm.analytics.ngp.repository_v3.SparkPipelineModelArtifact$SparkPipelineModelReader$$anonfun$openStream$5$$anonfun$apply$13.apply(MLRepositoryArtifact.scala:918)","physicalPlanDescription":"== Parsed Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data), ErrorIfExists, [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]\n+- AnalysisBarrier\n      +- Repartition 1, true\n         +- LocalRelation [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]\n\n== Analyzed Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data), ErrorIfExists, [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]\n+- Repartition 1, true\n   +- LocalRelation [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]\n\n== Optimized Logical Plan ==\nInsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data), ErrorIfExists, [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]\n+- Repartition 1, true\n   +- LocalRelation [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]\n\n== Physical Plan ==\nExecute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data), ErrorIfExists, [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]\n+- Exchange RoundRobinPartitioning(1)\n   +- LocalTableScan [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]","sparkPlanInfo":{"nodeName":"Execute InsertIntoHadoopFsRelationCommand","simpleString":"Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand file:/home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data, false, Parquet, Map(path -> /home/spark/shared/wml/repo/model_1551391033851/nodeEstimator/data), ErrorIfExists, [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]","children":[{"nodeName":"Exchange","simpleString":"Exchange RoundRobinPartitioning(1)","children":[{"nodeName":"LocalTableScan","simpleString":"LocalTableScan [numClasses#6330, numFeatures#6331, interceptVector#6332, coefficientMatrix#6333, isMultinomial#6334]","children":[],"metrics":[{"name":"number of output rows","accumulatorId":1724,"metricType":"sum"}]}],"metrics":[{"name":"data size total (min, med, max)","accumulatorId":1719,"metricType":"size"}]}],"metrics":[{"name":"number of written files","accumulatorId":1720,"metricType":"sum"},{"name":"bytes of written output","accumulatorId":1721,"metricType":"sum"},{"name":"number of output rows","accumulatorId":1722,"metricType":"sum"},{"name":"number of dynamic part","accumulatorId":1723,"metricType":"sum"}]}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":7}
